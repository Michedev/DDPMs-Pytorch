defaults:
  - model: unet_paper
  - scheduler: linear
  - dataset: mnist
  - optimizer: adam_ddpm
  - optional model_dataset: ${model}-${dataset}
  - optional model_scheduler: ${model}-${scheduler}

batch_size: 128
noise_steps: 4_000  # T
accelerator: null  # from pytorch-lightning, the hardware platform used to train the neural network
devices: null  # the devices to use in a given hardware platform (see argument above)
gradient_clip_val: 0.0  # gradient clip value - set to 0.0 to disable
gradient_clip_algorithm: norm  # gradient clip algorithm - either 'norm' or 'value'
ema: true  # exponential moving average
ema_decay: 0.99  # exponential moving average decay rate
early_stop: true  # stop training if the validation loss does not improve for patience epochs
patience: 10  # early stopping patience; set to -1 to disable
min_delta: 0.0  # minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.
ckpt: null  # path to checkpoint
seed: 1337  # random seed
freq_logging: 100  # frequency of logging
freq_logging_norm_grad: 100  # frequency of logging the norm of the gradient
batch_size_gen_images: 64  # batch size for generating images

hydra:
  run:
    dir: saved_models/${now:%Y_%m_%d_%H_%M_%S}  # where run train.py it will create under {current working directory}/saved_models a folder with the current date and time and it will be setted as new cwd
